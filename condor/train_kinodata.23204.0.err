wandb: Currently logged in as: joschka (nextaids). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/jgross/kinodata-docked-rescore/wandb/run-20230219_125204-34nafay5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-leaf-40
wandb: ⭐️ View project at https://wandb.ai/nextaids/kinodata-docked-rescore
wandb: 🚀 View run at https://wandb.ai/nextaids/kinodata-docked-rescore/runs/34nafay5
/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:285: UserWarning: Providing log_model=all requires wandb version >= 0.10.22 for logging associated model metadata.
Hint: Upgrade with `pip install --upgrade wandb`.
  rank_zero_warn(
/main/home/mambaforge/lib/python3.9/site-packages/torch_geometric/data/dataset.py:158: UserWarning: The `pre_filter` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-fitering technique, make sure to delete '{self.processed_dir}' first
  warnings.warn(
Auto select gpus: [0]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory ./lightning_logs/version_None/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-8ac7e914]

  | Name              | Type           | Params
-----------------------------------------------------
0 | initial_embedding | Embedding      | 25.6 K
1 | encoder           | GIN            | 526 K 
2 | readout           | SumAggregation | 0     
3 | prediction_head   | MLP            | 66.6 K
4 | criterion         | MSELoss        | 0     
-----------------------------------------------------
618 K     Trainable params
0         Non-trainable params
618 K     Total params
2.474     Total estimated model params size (MB)
/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:352: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
`Trainer.fit` stopped: `max_epochs=100` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            lr-AdamW █████████████████▅▅▅▅▅▅▅▅▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁
wandb:    train_loss_epoch █▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss_step ██▄▆▅▅▅▄▄▃▃▄▃▂▃▂▂▂▁▂▂▂▂▂▂▁▂▂▂▁▁▂▁▂▂▂▁▁▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            val_corr ▁▃▄▄▅▆▆▆▇▇▇▇▇▇█▇████████████████████████
wandb:             val_mae █▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 99
wandb:            lr-AdamW 5e-05
wandb:    train_loss_epoch 0.45162
wandb:     train_loss_step 0.24836
wandb: trainer/global_step 48599
wandb:            val_corr 0.74233
wandb: 
wandb: Synced curious-leaf-40: https://wandb.ai/nextaids/kinodata-docked-rescore/runs/34nafay5
wandb: Synced 5 W&B file(s), 101 media file(s), 38 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230219_125204-34nafay5/logs
